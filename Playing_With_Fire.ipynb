{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Playing With Fire",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agarapati/ml1/blob/master/Playing_With_Fire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFj9MIbF8dIK",
        "colab_type": "code",
        "outputId": "1e2c0a87-f8c5-4e8c-daa9-bc2fb65b55cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install pandas\n",
        "! pip install sodapy\n",
        "! pip install geopy\n",
        "! pip install numpy\n",
        "! pip install seaborn\n",
        "! pip install matplotlib\n",
        "! apt install proj-bin libproj-dev libgeos-dev\n",
        "! apt-get install libgeos-3.5.0\n",
        "! apt-get install libgeos-dev\n",
        "! pip install https://github.com/matplotlib/basemap/archive/master.zip\n",
        "! pip install pyproj==1.9.6\n",
        "! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.5)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "Collecting sodapy\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/e9/99b640c13544f03fc8d169fc99811d834a0d69ba5a69c68a3e891962ae5d/sodapy-1.5.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from sodapy) (2.21.0)\n",
            "Collecting future>=0.17.1 (from sodapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->sodapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->sodapy) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->sodapy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->sodapy) (2.8)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.17.1-cp36-none-any.whl size=488730 sha256=d1b7bb5a4be116f22e2ef8e9cfdc967c7c4de115c56a3403360ca71f77713cf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
            "Successfully built future\n",
            "Installing collected packages: future, sodapy\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed future-0.17.1 sodapy-1.5.4\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.6/dist-packages (1.17.0)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy) (1.49)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.3.1)\n",
            "Requirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.16.5)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.15.2->seaborn) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas>=0.15.2->seaborn) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.3->seaborn) (41.2.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.2.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgdal-doc\n",
            "The following NEW packages will be installed:\n",
            "  libgeos-dev libproj-dev proj-bin\n",
            "0 upgraded, 3 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 305 kB of archives.\n",
            "After this operation, 1,706 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgeos-dev amd64 3.6.2-1build2 [73.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libproj-dev amd64 4.9.3-2 [199 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 proj-bin amd64 4.9.3-2 [32.3 kB]\n",
            "Fetched 305 kB in 1s (241 kB/s)\n",
            "Selecting previously unselected package libgeos-dev.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../libgeos-dev_3.6.2-1build2_amd64.deb ...\n",
            "Unpacking libgeos-dev (3.6.2-1build2) ...\n",
            "Selecting previously unselected package libproj-dev:amd64.\n",
            "Preparing to unpack .../libproj-dev_4.9.3-2_amd64.deb ...\n",
            "Unpacking libproj-dev:amd64 (4.9.3-2) ...\n",
            "Selecting previously unselected package proj-bin.\n",
            "Preparing to unpack .../proj-bin_4.9.3-2_amd64.deb ...\n",
            "Unpacking proj-bin (4.9.3-2) ...\n",
            "Setting up libproj-dev:amd64 (4.9.3-2) ...\n",
            "Setting up libgeos-dev (3.6.2-1build2) ...\n",
            "Setting up proj-bin (4.9.3-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package libgeos-3.5.0\n",
            "E: Couldn't find any package by glob 'libgeos-3.5.0'\n",
            "E: Couldn't find any package by regex 'libgeos-3.5.0'\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libgeos-dev is already the newest version (3.6.2-1build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
            "Collecting https://github.com/matplotlib/basemap/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/matplotlib/basemap/archive/master.zip\n",
            "\u001b[K     | 133.2MB 644kB/s\n",
            "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.1,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from basemap==1.2.1) (3.0.3)\n",
            "Requirement already satisfied: numpy>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from basemap==1.2.1) (1.16.5)\n",
            "Collecting pyproj>=1.9.3 (from basemap==1.2.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/37/86f8a661cf252ff16a1a11b2c2a452e0d19aebf8934cc70e9a95d2d038be/pyproj-2.3.1-cp36-cp36m-manylinux1_x86_64.whl (9.8MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8MB 2.7MB/s \n",
            "\u001b[?25hCollecting pyshp>=1.2.0 (from basemap==1.2.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/16/3bf15aa864fb77845fab8007eda22c2bd67bd6c1fd13496df452c8c43621/pyshp-2.1.0.tar.gz (215kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from basemap==1.2.1) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (2.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.1,>=1.0.0->basemap==1.2.1) (41.2.0)\n",
            "Building wheels for collected packages: basemap, pyshp\n",
            "  Building wheel for basemap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for basemap: filename=basemap-1.2.1-cp36-cp36m-linux_x86_64.whl size=121756021 sha256=67fe85c0d17f2d1386b6135e112f95aba9ccbea6983c143cfede336d10269199\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fzmosvsk/wheels/98/4a/fc/ce719b75d97e646645c225f3332b1b217536100314922e9572\n",
            "  Building wheel for pyshp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyshp: filename=pyshp-2.1.0-cp36-none-any.whl size=32607 sha256=747ed17953f07faf17609ee8c447d1dfc16084949542d6d7807d740b07c833ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/0c/de/321b5192ad416b328975a2f0385f72c64db4656501eba7cc1a\n",
            "Successfully built basemap pyshp\n",
            "Installing collected packages: pyproj, pyshp, basemap\n",
            "Successfully installed basemap-1.2.1 pyproj-2.3.1 pyshp-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting pyproj==1.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/8c/1da0580f334718e04f8bbf74f0515a7fb8185ff96b2560ce080c11aa145b/pyproj-1.9.6.tar.gz (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyproj\n",
            "  Building wheel for pyproj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyproj: filename=pyproj-1.9.6-cp36-cp36m-linux_x86_64.whl size=3702055 sha256=f20c4382d683d409ab68de373fac09fa290f0dc3ecd2371b799349e55b0a9b9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/cd/b1/a2d6430f74c7a778a43d62f78bec109ca69c732dc9b929142a\n",
            "Successfully built pyproj\n",
            "Installing collected packages: pyproj\n",
            "  Found existing installation: pyproj 2.3.1\n",
            "    Uninstalling pyproj-2.3.1:\n",
            "      Successfully uninstalled pyproj-2.3.1\n",
            "Successfully installed pyproj-1.9.6\n",
            "Collecting https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/pandas-profiling/pandas-profiling/archive/master.zip\n",
            "\u001b[K     | 8.4MB 11.5MB/s\n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling==2.3.0) (0.24.2)\n",
            "Requirement already satisfied: matplotlib>=1.4 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling==2.3.0) (3.0.3)\n",
            "Requirement already satisfied: jinja2>=2.8 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling==2.3.0) (2.10.1)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.6/dist-packages (from pandas-profiling==2.3.0) (0.4.2)\n",
            "Collecting htmlmin>=0.1.12 (from pandas-profiling==2.3.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
            "Collecting phik>=0.9.8 (from pandas-profiling==2.3.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/ad/24a16fa4ba612fb96a3c4bb115a5b9741483f53b66d3d3afd987f20fa227/phik-0.9.8-py3-none-any.whl (606kB)\n",
            "\u001b[K     |████████████████████████████████| 614kB 4.2MB/s \n",
            "\u001b[?25hCollecting confuse>=1.0.0 (from pandas-profiling==2.3.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/6f/90e860cba937c174d8b3775729ccc6377eb91f52ad4eeb008e7252a3646d/confuse-1.0.0.tar.gz\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.6/dist-packages (from pandas-profiling==2.3.0) (3.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pandas-profiling==2.3.0) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pandas-profiling==2.3.0) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->pandas-profiling==2.3.0) (1.16.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling==2.3.0) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4->pandas-profiling==2.3.0) (2.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.8->pandas-profiling==2.3.0) (1.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from missingno>=0.4.2->pandas-profiling==2.3.0) (1.3.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from missingno>=0.4.2->pandas-profiling==2.3.0) (0.9.0)\n",
            "Requirement already satisfied: numba>=0.38.1 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling==2.3.0) (0.40.1)\n",
            "Collecting pytest-pylint>=0.13.0 (from phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/64/dc/6f35f114844fb12e38d60c4f3d2441a55baff7043ad4e013777dff55746c/pytest_pylint-0.14.1-py3-none-any.whl\n",
            "Collecting pytest>=4.0.2 (from phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/19/d5f71752f71451ccc5ed5f6739e9da4a235f38783fdaf3629cae41b2ca7b/pytest-5.1.2-py3-none-any.whl (224kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nbconvert>=5.3.1 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling==2.3.0) (5.6.0)\n",
            "Requirement already satisfied: jupyter-client>=5.2.3 in /usr/local/lib/python3.6/dist-packages (from phik>=0.9.8->pandas-profiling==2.3.0) (5.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from confuse>=1.0.0->pandas-profiling==2.3.0) (3.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas>=0.19->pandas-profiling==2.3.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4->pandas-profiling==2.3.0) (41.2.0)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.1->phik>=0.9.8->pandas-profiling==2.3.0) (0.29.0)\n",
            "Collecting pylint>=1.4.5 (from pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/c2/b3f73f4ac008bef6e75bca4992f3963b3f85942e0277237721ef1c151f0d/pylint-2.3.1-py3-none-any.whl (765kB)\n",
            "\u001b[K     |████████████████████████████████| 768kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0) (1.3.0)\n",
            "Collecting pluggy<1.0,>=0.12 (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/92/c7/48439f7d5fd6bddb4c04b850bb862b42e3e2b98570040dfaf68aedd8114b/pluggy-0.13.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0) (7.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0) (0.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0) (0.20)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0) (19.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0) (19.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (0.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (0.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (4.3.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (0.6.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (2.1.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (4.5.0)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (4.4.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (1.4.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling==2.3.0) (4.5.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.3->phik>=0.9.8->pandas-profiling==2.3.0) (17.0.0)\n",
            "Collecting isort<5,>=4.2.5 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 21.8MB/s \n",
            "\u001b[?25hCollecting astroid<3,>=2.2.0 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/ad/7221a62a2dbce5c3b8c57fd18e1052c7331adc19b3f27f1561aa6e620db2/astroid-2.2.5-py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 36.4MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6 (from pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=4.0.2->phik>=0.9.8->pandas-profiling==2.3.0) (0.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (4.4.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (0.5.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert>=5.3.1->phik>=0.9.8->pandas-profiling==2.3.0) (2.6.0)\n",
            "Collecting lazy-object-proxy (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/26/534a6d32572a9dbca11619321535c0a7ab34688545d9d67c2c204b9e3a3d/lazy_object_proxy-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.6MB/s \n",
            "\u001b[?25hCollecting typed-ast>=1.3.0; implementation_name == \"cpython\" (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling==2.3.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/d3/9d1802c161626d0278bafb1ffb32f76b9d01e123881bbf9d91e8ccf28e18/typed_ast-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (736kB)\n",
            "\u001b[K     |████████████████████████████████| 737kB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from astroid<3,>=2.2.0->pylint>=1.4.5->pytest-pylint>=0.13.0->phik>=0.9.8->pandas-profiling==2.3.0) (1.11.2)\n",
            "Building wheels for collected packages: pandas-profiling, htmlmin, confuse\n",
            "  Building wheel for pandas-profiling (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-profiling: filename=pandas_profiling-2.3.0-py2.py3-none-any.whl size=144604 sha256=b593531105fe7b9d8de8fa6dbeeb67d22494bb6e4b1704e639dbfc304667d7e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x91x2qjz/wheels/56/c2/dd/8d945b0443c35df7d5f62fa9e9ae105a2d8b286302b92e0109\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-cp36-none-any.whl size=27084 sha256=823a7e4071443030c55c9966410bb497f0cdd218ecc4cb4fa284efed6d3458b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
            "  Building wheel for confuse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for confuse: filename=confuse-1.0.0-cp36-none-any.whl size=17486 sha256=7ae40b6246b32513626643d48f73fa01c722ba20d4611e79e75824ff338ce519\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/b2/96/2074eee7dbf7b7df69d004c9b6ac4e32dad04fb7666cf943bd\n",
            "Successfully built pandas-profiling htmlmin confuse\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: htmlmin, pluggy, pytest, isort, lazy-object-proxy, typed-ast, astroid, mccabe, pylint, pytest-pylint, phik, confuse, pandas-profiling\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "  Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "Successfully installed astroid-2.2.5 confuse-1.0.0 htmlmin-0.1.12 isort-4.3.21 lazy-object-proxy-1.4.2 mccabe-0.6.1 pandas-profiling-2.3.0 phik-0.9.8 pluggy-0.13.0 pylint-2.3.1 pytest-5.1.2 pytest-pylint-0.14.1 typed-ast-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_d48CtbH_BG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RloBmFUi17i4",
        "colab_type": "text"
      },
      "source": [
        "# Laboratory 1: Examining New York City (NYC) Fire Incidents\n",
        "\n",
        "### Team Members: Chase Henderson, Aditya Garapati, Carl Walenciak, Brian Waite\n",
        "\n",
        "\n",
        "## Business Understanding\n",
        "\n",
        "### Data General Description: \n",
        "\n",
        "The Fire Department of New York City (FDNY) collects data on all incidents handled by non-EMS units including fire, medical and non-fire emergencies. This data is more broadly collected in support of the New York Fire Incident Reporting System (NYFIRS) and the National Fire Incident Reporting System (NFIRS). According to the New York City Open Data platform (https://data.cityofnewyork.us) NFIRS data was originally intended to aid in understanding the nature and causes of fire, civilian fire casualties and firefighter injuries, but has since expanded to collect information on all incidents to which fire units respond. [1]\n",
        "\n",
        "Although FDNY Incident data is available from 2013 through June, 2018, the total number of records for that period is approximately 2.5 Million and represented a challenge to computing resources available to the team. In order to limit the size of the data set, but still achieve the research objectives, the team downselected the data set to include only fire incident records from the one year period beginning 07-01-2017 at 12AM through 06-30-2018 at midnight. The resultant set contains 337,210 records with 25 original feature columns.\n",
        "\n",
        "### Data Determined Out of Scope: \n",
        "\n",
        "As indicated above, the original FDNY Incident data set contains a variety of incident types including non-fire medical emergencies, gas leaks, support to police activity and wide variety of other events. In order to deliver meaningful analytic outcomes and focus on specific challenge problems, we down scoped our data set to include only fire-related incidents (indicated within the data set as 100-series (1XX) incidents. This reduces the final data set of interesting events to 26,339 events that are just fires. \n",
        "\n",
        "### Supplemental Data: \n",
        "\n",
        "In order to enable the team to ask more robust questions of interest of the primary data, we sought additional associated or relevant data to supplement our analysis. This includes: \n",
        "\n",
        "* Firebox Location Data: Each incident within the primary data set is linked to a specific firebox. New York City's unique infrastructure includes a series of over 15,000 on-street fireboxes that allow easy access to emergency services. While many of these are in varying states of repair, the FDNY uses them as geographic reference points for the dispatch of units and the records kept in the primary dataset. The team sought and identified geo-reference data to improve the utility of the firebox field in the primary data set. This was obtained from reference [2]. Although this data is somewhat dated -- last updated in 2008 -- the age of the NYC Firebox system and the immobility of the devices makes us confident that the location information is unchanged. Data for 16,284 fireboxes is listed. \n",
        "\n",
        "* Firehouse Location Data: The team also sought and identified a complete listing of the name, type, and geo-reference data of all 213 firehouses in the boroughs of NYC. [3]\n",
        "\n",
        "* Precipitation Data: Thinking that road conditions, particularly associated with heavy rainfall or snow accumulation could impact response times, we sought and obtained weather data for New York City's Central Park for the entire period of the data set. These values are \"daily\" and are linked in the final data set with the date of the incident. \n",
        "\n",
        "### Analytic Questions of Interest: \n",
        "\n",
        "Using this publicly available data set, the team has identified a number of potential analytic questions of interest that could potentially provide value to FDNY and NYC decision makers regarding the allocation of financial and personnel resources. Specific questions of interest could include: \n",
        "\n",
        "* Identification of regions of New York City where fire incidents are most likely to occur. \n",
        "* Prediction of the response time and/or total incident duration based on available features. \n",
        "* Identify trends within the data set that might indicate increased periods of risk. \n",
        "\n",
        "These insights could specifically be used to determine which fire stations should receive additional personnel or equipment resources or design staffing schedules to coincide with the most likely periods of fire related activity. \n",
        "\n",
        "Further, this analysis may help to aid in the identification of problem neighborhoods where increased inspection activity or public education campaigns could help to contribute to additional fire prevention measures being deployed in a proactive fashion by city decision makers. \n",
        "\n",
        "### Measuring Outcomes from the Data: \n",
        "\n",
        "The team believes that this data set can support a variety of analytic methodologies and provide interesting answers to the question of interest above. Techniques available will include both classification and regression based models. In order to ensure validity of results, all evaluations of data will be on a 70/30 train/test split of the data with test data being withheld for final testing and not made available for model improvement at any stage of the analysis. \n",
        "\n",
        "Below we describe our evaluative criteria for each type of model anticipated: \n",
        "\n",
        "In all cases, our models will be evaluated on whether the findings are logical and representative of reasonable outcomes given the data provided and are consistent with the exploratory data analysis. \n",
        "\n",
        "*Linear Regression:* \n",
        "\n",
        "Feature selection for inclusion in the model will likely make use of feature down-selection techniques including forward, backward, LASSO or ELASTIC-NET selection techniques with Bayesian Information Criteria, Adjusted R-Squared, and Residual Sum of Squares acting as the primary selection criteria. These criteria will also be used to evaluate the model fit in comparison to the test data set. \n",
        "\n",
        "*Classification:*\n",
        "\n",
        "Evaluation of classification models will be through the use of confusion matrix based statistics including accuracy, sensitivity and specificity. In addition, the Area Under the Curve (AUC) assessments using Receiver Operating Characteristic (ROC) curve analysis will be used to compare model performance and to evaluate the performance of any final selected model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZLTQy7Y9Smp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "34903140-d34a-4831-a08c-e160d899ddd9"
      },
      "source": [
        "# Imports section\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from geopy import distance\n",
        "from sodapy import Socrata\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "import missingno as msno\n",
        "import requests\n",
        "from google.colab import drive\n",
        "import datetime as dt\n",
        "import pandas_profiling\n",
        "from IPython.display import display, HTML\n",
        "import os\n",
        "import shapefile\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e4a4800e50b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgeopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msodapy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSocrata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmissingno\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sodapy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Kg9EUM8-gB",
        "colab_type": "code",
        "outputId": "374a0441-4c5c-4b2a-82aa-c83617e17dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "# Data Import From City of New York\n",
        "# Ignore warning regarding API limitations as that is only regarding programming requests at speed. \n",
        "\n",
        "# Unauthenticated client only works with public data sets. Note 'None'\n",
        "# in place of application token, and no username or password:\n",
        "client = Socrata(\"data.cityofnewyork.us\", None)\n",
        "\n",
        "# Example authenticated client (needed for non-public datasets):\n",
        "# client = Socrata(data.cityofnewyork.us,\n",
        "#                  MyAppToken,\n",
        "#                  userame=\"user@example.com\",\n",
        "#                  password=\"AFakePassword\")\n",
        "\n",
        "# First 2000 results, returned as JSON from API / converted to Python list of\n",
        "# dictionaries by sodapy.\n",
        "results = client.get(\"tm6d-hbzd\", limit = 500000)#limit=300000000)\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "results_df = pd.DataFrame.from_records(results)\n",
        "\n",
        "#Get the firehouse location information\n",
        "firehouse_info = client.get(\"hc8x-tcnd\", limit=10000)\n",
        "\n",
        "firehouse_df = pd.DataFrame.from_records(firehouse_info)\n",
        "firehouse_df = firehouse_df.dropna()\n",
        "\n",
        "#Set up Google Drive access to additional data stored there\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Get the firebox location information\n",
        "firebox_locs = pd.read_csv(\"/content/drive/My Drive/Fire Data/Fire Boxes.csv\", names=['LONG','LAT','fire_box','nearest_intersection'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbxib1QuH2cL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get the precipitation information for NYC on the dates of interest\n",
        "precip = pd.read_csv(\"/content/drive/My Drive/Fire Data/Rain_Snow.csv\")\n",
        "precip['DATE'] = pd.to_datetime(precip['DATE'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8WdxQM39mdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert dates to date time objects\n",
        "results_df['arrival_date_time']=pd.to_datetime(results_df['arrival_date_time'])\n",
        "results_df['incident_date_time']=pd.to_datetime(results_df['incident_date_time'])\n",
        "results_df['last_unit_cleared_date_time']=pd.to_datetime(results_df['last_unit_cleared_date_time'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVnrh6Ba9yen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Limit results to the most recent 1 year time frame available -- July 2017 through June 2018\n",
        "results_df = results_df[results_df['incident_date_time'] > pd.to_datetime('2017-07-01 00:00:00')]\n",
        "\n",
        "#Calculate new feature \"response_time\" that is the arrival time of units on scene minus the time the incident occured. \n",
        "#Convert to a numeric type and convert units to minutes. \n",
        "results_df['response_time'] = (results_df['arrival_date_time'] - results_df['incident_date_time'])\n",
        "results_df['response_time'] = pd.to_numeric(results_df['response_time'])\n",
        "results_df['response_time'] = results_df['response_time']/ 60000000000\n",
        "\n",
        "#Convert other data elements to the proper type\n",
        "results_df['units_onscene'] = pd.to_numeric(results_df['units_onscene'])\n",
        "results_df['total_incident_duration'] = pd.to_numeric(results_df['total_incident_duration'])\n",
        "results_df['story_fire_origin_count'] = pd.to_numeric(results_df['story_fire_origin_count'])\n",
        "results_df['fire_box'] = results_df['fire_box'].astype('str')\n",
        "\n",
        "#Convert firehouse values\n",
        "firehouse_df['latitude'] = pd.to_numeric(firehouse_df['latitude'])\n",
        "firehouse_df['longitude'] = pd.to_numeric(firehouse_df['longitude'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaTVw4MR9-Ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add the borough code to the firebox column to enable merging with the firebox location data set\n",
        "results_df.loc[results_df['borough_desc'].str.match('1 - Manhattan'), 'fire_box'] = 'M' + results_df.fire_box\n",
        "results_df.loc[results_df['borough_desc'].str.match('2 - Bronx'), 'fire_box'] = 'X' + results_df.fire_box\n",
        "results_df.loc[results_df['borough_desc'].str.match('3 - Staten Island'), 'fire_box'] = 'R' + results_df.fire_box\n",
        "results_df.loc[results_df['borough_desc'].str.match('4 - Brooklyn'), 'fire_box'] = 'B' + results_df.fire_box\n",
        "results_df.loc[results_df['borough_desc'].str.match('5 - Queens'), 'fire_box'] = 'Q' + results_df.fire_box"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyz-CVU74Sds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Merge the fire_box dataframe with the overall dataframe\n",
        "final_df = pd.merge(results_df, firebox_locs, on = 'fire_box')\n",
        "final_df['LAT'] = pd.to_numeric(final_df['LAT'])\n",
        "final_df['LONG'] = pd.to_numeric(final_df['LONG'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk2n9Q5DTFme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reduce only to fire types\n",
        "final_df[['incident_code','incident_desc']]=final_df.incident_type_desc.str.split('-', expand=True, n=1)\n",
        "final_df.incident_code.str.strip()\n",
        "final_df.incident_desc.str.strip()\n",
        "final_df['incident_code'] = pd.to_numeric(final_df['incident_code'],errors='coerce')\n",
        "final_df = final_df[final_df.incident_code < 200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6beuMpM05XYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def findNearestFirestation (Lat, Long):\n",
        "    minDistance = 1000\n",
        "    for fireLat, fireLong in zip(firehouse_df.latitude, firehouse_df.longitude):\n",
        "        distanceFire = distance.distance((Lat, Long), (fireLat,fireLong)).miles\n",
        "        if distanceFire < minDistance: \n",
        "            minDistance = distanceFire\n",
        "    return minDistance\n",
        "\n",
        "# #Uncomment to run this code, but takes multiple hours to calculate distance for each point. \n",
        "# #Get the nearest fire station\n",
        "# #This code takes a long time to run because it needs to calculate the distance for each firebox to all fire stations and then select the minimum.\n",
        "# final_df['Distance_To_Nearest_Station'] = 0\n",
        "# i = 0\n",
        "# for lat, long in zip (final_df.LAT, final_df.LONG): \n",
        "#     dist = findNearestFirestation(lat,long)\n",
        "#     final_df['Distance_To_Nearest_Station'].iloc[i] = dist\n",
        "#     i = i + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sbPt1O9e1_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the final file to a pickle file so that we don't have to re-run\n",
        "#Uncommenting will overwrite the existing data file and will take time to generate if not running the distance to fire station code above. \n",
        "#final_df.to_pickle('/content/drive/My Drive/Fire Data/fireData_July17.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1bmr9-efFOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHREC9dYef4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#All code above this line is to create the initial data set. Due to size and the time required to generate the \"Distance_To_Nearest_Station\" feature, we have run and stored the data file \n",
        "#of line in a \"pickle\" file to enable rapid analysis by the team. This file is read in here and retains all of the data type associations generated above. \n",
        "\n",
        "final_df = pd.read_pickle('/content/drive/My Drive/Fire Data/fireData_July17.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4QSrVtBJTyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop the unnecessary precip data\n",
        "precip.drop(['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION'], axis=1, inplace=True)\n",
        "\n",
        "#Pull out the date to support merge\n",
        "final_df['DATE'] = final_df.apply (lambda row: row['incident_date_time'].date(), axis=1)\n",
        "\n",
        "#Some magical required type conversion\n",
        "final_df['DATE'] = pd.to_datetime(final_df['DATE'])\n",
        "precip['DATE'] = pd.to_datetime(precip['DATE'])\n",
        "\n",
        "#Merge the precipitation data with the final data frame\n",
        "final_df = pd.merge(final_df, precip, on = 'DATE')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-cleVD1Z5Sd",
        "colab_type": "text"
      },
      "source": [
        "## Data Meaning and Type Description "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiW8LP4cQ_h4",
        "colab_type": "text"
      },
      "source": [
        "The below variable listing describes the name, description, type, and unit values for each of the variables retained in the dataframe to be used to support this analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgOnLvbZ1QLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Put together a dataframe to store the variable descriptions for ease of use. \n",
        "\n",
        "#Get the variable names from the column header\n",
        "variables = final_df.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0i_8Sq2X1ch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsbAPWK81Xax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert to a dataframe\n",
        "variable_df = pd.DataFrame(variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbIjvvtI1hON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Build additional columns in the dataframe with the description, type, and units. \n",
        "\n",
        "variable_df['Description'] = ['The code and description of the first action taken by units on the scene.',\n",
        "                              'The code and description of the second action taken by units on the scene.',\n",
        "                              'The code and description of the third action taken by units on the scene.',\n",
        "                              'Indicator for whether an Automatic Extinguishing System (AES) is present.',\n",
        "                              'Arrival Date and Time of the first unit on the scene.',\n",
        "                              'The borough in which the incident occured. One of: Bronx, Brooklyn, Manhattan, Queens, Staten Island.',\n",
        "                              'Indicator for whether a Carbon Monoxide Detector was present.',\n",
        "                              'Indicator for whether a Smoke Detector was present.',\n",
        "                              'The emergency firebox where the incident occured and to which units responded. Character represents the borough and numeric value is unique ID.',\n",
        "                              'Indicator for whether the fire began below grade or not.',\n",
        "                              'Description of how far the fire spread from the object of origin.',\n",
        "                              'The floor of the building on which the incident occured.',\n",
        "                              'Highest level of alarm the incident achieves. Evaluated after completion.',\n",
        "                              'Unique incident key value to identify the incident. Not used for analysis beyond identification of records.',\n",
        "                              'Date and Time on which incident occured. Assumed to be reported in local time.',\n",
        "                              'Incident type code (100-series for fire) and specific description of type of fire.',\n",
        "                              'Date and time the last responding unit left the scene of the incident.',\n",
        "                              'The code and description of the type of street or building where the incident took place',\n",
        "                              'Indicator of whether a standpipe (hydrant) was in the vicinity of where the fire began.',\n",
        "                              'Story on which the fire originated.',\n",
        "                              'Name of the street where the incident took place.',\n",
        "                              'The total number of seconds from whe then incident was created to when the incident was closed.',\n",
        "                              'Total number of units that arrived on scene.',\n",
        "                              'Zip code where the incident occured.',\n",
        "                              'An engineered feature derived from arrival time of first units on scene and the time the incident occured.',\n",
        "                              'Longitude of the location where the incident occured.',\n",
        "                              'Latitude of the location where the incident occured.',\n",
        "                              'Nearest intersection to the firebox where the incident was reported.',\n",
        "                              'Unique incident type code parsed from incident code and description above.',\n",
        "                              'Incident description parsed from incident code and description above.',\n",
        "                              'An engineered feature derived by measuring the distance between the nearest firehouse location and the firebox. Straight line distance using Mercator projection.',\n",
        "                              'Month, Day, and Year of incident.',\n",
        "                              'Precipitation amount as measured in Central Park on date of the incident.',\n",
        "                              'Amount of precipitation that was recorded as snowfall.',\n",
        "                              'Amount of snow accumulation reported on the day of the incident.']\n",
        "variable_df['Type'] = ['Categorical',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'Datetime',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'String',\n",
        "                       'Datetime',\n",
        "                       'Categorical',\n",
        "                       'Datetime',\n",
        "                       'Categorical',\n",
        "                       'Categorical',\n",
        "                       'String',\n",
        "                       'Numeric',\n",
        "                       'Numeric',\n",
        "                       'Numeric',\n",
        "                       'Categorical',\n",
        "                       'Numeric',\n",
        "                       'Numeric',\n",
        "                       'Numeric',\n",
        "                       'String',\n",
        "                       'Categorical',\n",
        "                       'String',\n",
        "                       'Numeric',\n",
        "                       'Datetime',\n",
        "                       'Numeric',\n",
        "                       'Numeric',\n",
        "                       'Numeric'\n",
        "                                            \n",
        "                       ]\n",
        "\n",
        "variable_df['Units'] = ['N/A',\n",
        "                        'N/A',\n",
        "                        'N/A',\n",
        "                        'N/A',\n",
        "                        'N/A',\n",
        "                        'N/A',\n",
        "                        'N/A',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                        'N/A',\n",
        "                        'N/A',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                        'N/A',\n",
        "                        'Floors',\n",
        "                       'N/A',\n",
        "                       'Seconds',\n",
        "                       'Number of Fire Response Units',\n",
        "                       'N/A',\n",
        "                       'Minutes',\n",
        "                       'Degrees Longitude',\n",
        "                       'Degrees Latitude',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                       'N/A',\n",
        "                       'Miles',\n",
        "                       'N/A',\n",
        "                       'Inches',\n",
        "                       'Inches',\n",
        "                       'Inches',\n",
        "                                     \n",
        "                       ]\n",
        "variable_df.rename(columns={0:'Feature'},inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpaN0PVsQSp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Display and ensure that the full description is shown. \n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "display(HTML(variable_df.to_html()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jf0tRldQIXy",
        "colab_type": "text"
      },
      "source": [
        "## Profile Report on the New York City fire dataset. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7ZoU67RSN4d",
        "colab_type": "text"
      },
      "source": [
        "The below report provides an opportunity to interact directly with the datatypes and identify attributes, ranges, summary statistics, correlation and missing values of features in the dataset. Specific areas of interest identified by the team in this reporting or other exploratory data analysis are called out below this report and addressed as needed. As an additional supplement to this and the above definitions, a reference to NYC Fire Dispatch codes is available at: http://www.fdnewyork.com/aa.asp. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns6DorKwQPwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#generate the report\n",
        "profile = final_df.profile_report(style={'full_width':True}, correlations={\"cramers\": False})\n",
        "profile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cElpxsj2Y59A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzLIzI2Ecajm",
        "colab_type": "text"
      },
      "source": [
        "## Data Quality: Addressing Missing Values, Outliers, Duplicates and Other Issues Identified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q11o-hgUrUa",
        "colab_type": "text"
      },
      "source": [
        "### Missing value analysis: \n",
        "\n",
        "Looking at the above profile report and on subsequent review of the data, we could easily identify large portions of the data that are missing or are appropriately not reported. A matrix plot helps us to visualize where the large portions of missing data are held. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNbxD7G-VKcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Display the missing value matrix plot\n",
        "msno.matrix(final_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GAjgVxKVR0k",
        "colab_type": "text"
      },
      "source": [
        "### Unlikely to be able to impute. \n",
        "\n",
        "As we can see from the plot above, a number of the columns have significant missing values (indicated by whitespace). Many of these values are likely due to under-reporting by the participating fire units in the incident response after the fact or due to non-applicability of the field to the specific incident type reported. \n",
        "\n",
        "Example columns with significant missing values include `co_detector_presence_desc` and `detector_presence_desc` which indicate whether a carbon-monoxide or regular smoke detector were reported present at the scene. While this could potentially impact the rate at which a fire spreads or the scale of the fire prior to being reported, we do not believe we have enough of these values to make valid inferences and that they do not more broadly impact our questions of interest.\n",
        "\n",
        "### Imputation Possible. \n",
        "\n",
        "For some other variables, we do believe that imputation of the missing values or values that were engineered but are inaccurate due to other missing values. One example is the `response_time` variable we created above. Because there are missing values for some of the arrival times, the response time variable is negative, which we know cannot be possible. In the following code, we seek to impute these values while adjusting for the borough in which the incident occurs to retain some fidelity of the data. \n",
        "\n",
        "Additionally, the `units_onscene` variable has relatively few missing values. We felt that we could confidently impute these values according to borough, response time, and the incident type, thinking that similar units would be called to respond to similar incidents. This code is also below. \n",
        "\n",
        "#### Imputation of Response Time: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZTS8zMJanRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Response Time Cross Reference\n",
        "filter_to = final_df['response_time'] > 0\n",
        "xref_resp_time = final_df[filter_to][['borough_desc', 'response_time']]\n",
        "xref_resp_time = xref_resp_time.rename(columns = {\"response_time\":\"resp_time_mean\"})\n",
        "xref_resp_time = pd.DataFrame(xref_resp_time.groupby('borough_desc').mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozx0tzgJanR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge Data with Mean XRef\n",
        "final_df = final_df.merge(xref_resp_time, how = 'left', on = 'borough_desc', indicator = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X-kNSNjanR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set On Scene to XXXXX where Null\n",
        "filter_to = final_df['response_time'] <= 0\n",
        "final_df.loc[filter_to, 'response_time'] = final_df[filter_to]['resp_time_mean']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwqwe7fLanR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df = final_df.drop(['resp_time_mean', '_merge'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ot6bt3MiFz6",
        "colab_type": "text"
      },
      "source": [
        "#### Imputation of Units Onscene"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pam7MP1TanRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Units Onscene Cross Reference\n",
        "filter_to = final_df['units_onscene'].isnull()== False\n",
        "xref_units_onscene = final_df[filter_to][['borough_desc','incident_type_desc', 'units_onscene']]\n",
        "xref_units_onscene = xref_units_onscene.rename(columns = {\"units_onscene\":\"onscene_mean\"})\n",
        "xref_units_onscene = pd.DataFrame(xref_units_onscene.groupby('incident_type_desc').mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "537OXspOanRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge Data with Mean XRef\n",
        "final_df = final_df.merge(xref_units_onscene, how = 'left', on = 'incident_type_desc', indicator = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly7mIWTzanRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set On Scene to XXXXX where Null\n",
        "filter_to = final_df['units_onscene'].isnull()\n",
        "final_df.loc[filter_to, 'units_onscene'] = final_df[filter_to]['onscene_mean']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EsVFlbfanRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df = final_df.drop(['onscene_mean', '_merge'], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA_WIfyKekb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gd0C2VtekzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiIvDKp2cevn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Down sampling code no longer required. \n",
        "\n",
        "# sample_size = 50000\n",
        "# sampled_df = pd.DataFrame()\n",
        "\n",
        "# for i in boroughs:\n",
        "#     #Filter to Borough\n",
        "#     temp_df = final_df[final_df['borough_desc'] == i]\n",
        "    \n",
        "#     for x in months:\n",
        "#         #Filter to Month\n",
        "#         temp_df2 = temp_df[temp_df['incident_month'] == x]\n",
        "        \n",
        "#         #Get Proportional Sample Size\n",
        "#         num_of_samples = sample_size * (temp_df2.shape[0]/final_df.shape[0])\n",
        "        \n",
        "#         #Sample Data\n",
        "#         temp_sampled = temp_df2.sample(n = round(num_of_samples), random_state = 123)\n",
        "        \n",
        "#         #Assign to Dataframe\n",
        "#         sampled_df = pd.concat([sampled_df, temp_sampled])\n",
        "        \n",
        "# #store the sampled dataframe back as final_df\n",
        "# final_df = sampled_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L37IgQJyecgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Event count distribution after sampling\n",
        "# sns.countplot(x=\"borough_desc\", hue='incident_month', data=final_df)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJGPOg6bSx-I",
        "colab_type": "text"
      },
      "source": [
        "## Geographically Identified Missing Values\n",
        "\n",
        "Based on the initial description of the data, we assumed that the data set contained incident reporting for the full coverage area of all five NYC boroughs for the reporting period examined. When we plotted the data, however, we identified some significant missing values and outliers that we were not expecting to see. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQNkWmrbsgfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate a plot of the incidents in the data set by borough\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "m = Basemap(projection='tmerc',area_thresh = 0.1, resolution='f', llcrnrlon = final_df.LONG.min(), llcrnrlat = final_df.LAT.min(),\n",
        "urcrnrlon = final_df.LONG.max()+.03, urcrnrlat = final_df.LAT.max(), lat_0 = final_df.LAT.mean(), lon_0 = final_df.LONG.mean())\n",
        "m.drawmapboundary(fill_color='lightskyblue')\n",
        "m.fillcontinents(color='linen',lake_color='lightskyblue')\n",
        "m.drawcoastlines(color='gray')\n",
        "m.drawrivers(color='lightskyblue')\n",
        "#Fill the globe with a blue color \n",
        "\n",
        "#Fill the continents with the land color\n",
        "\n",
        "#Include this for the building footprint map\n",
        "#m.readshapefile(shpfile, 'metro', linewidth=.15)\n",
        "x, y = m(final_df.LONG.tolist(), final_df.LAT.tolist())\n",
        "sns.scatterplot(x,y, hue = final_df.borough_desc, zorder = 15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U--RXs8jthZ4",
        "colab_type": "text"
      },
      "source": [
        "#### Missing Regions\n",
        "\n",
        "As is visible above, we can see that areas in the Northern portion of Staten Island, the Southern end of Manhattan (South of Central Park), the area between Queens and Brooklyn and the Southern portion of Queens all appear to have large regions missing. Looking at maps, some of these areas can be explained by the presence of parks or other undeveloped space, but the majority of the large missing regions are simply missing from the data set. An overlay of the street maps for the region helps to demonstrate this. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRNlC5-PuefV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import shape file for road map\n",
        "shpfile = os.path.expanduser('/content/drive/My Drive/Fire Data/Maps/nyc2')\n",
        "sf = shapefile.Reader(shpfile)\n",
        "\n",
        "#Generate a plot of the incidents in the data set by borough\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "m = Basemap(projection='tmerc',area_thresh = 0.1, resolution='f', llcrnrlon = final_df.LONG.min(), llcrnrlat = final_df.LAT.min(),\n",
        "urcrnrlon = final_df.LONG.max()+.03, urcrnrlat = final_df.LAT.max(), lat_0 = final_df.LAT.mean(), lon_0 = final_df.LONG.mean())\n",
        "m.drawmapboundary(fill_color='lightskyblue')\n",
        "m.fillcontinents(color='linen',lake_color='lightskyblue')\n",
        "m.drawcoastlines(color='gray')\n",
        "m.drawrivers(color='lightskyblue')\n",
        "#Fill the globe with a blue color \n",
        "\n",
        "#Fill the continents with the land color\n",
        "\n",
        "#Include this for the building footprint map\n",
        "m.readshapefile(shpfile, 'metro', linewidth=.15)\n",
        "x, y = m(final_df.LONG.tolist(), final_df.LAT.tolist())\n",
        "sns.scatterplot(x,y, hue = final_df.borough_desc, zorder = 15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77aGp1kzunPz",
        "colab_type": "text"
      },
      "source": [
        "While not ideal, we do not believe that the absence of these regions will significantly impact our findings. Each borough is proportionally represented in the data for each month in the data set. Staten Island appears to be a small portion, but is by population the smallest Borough in NYC. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2BDNABYu9ie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set up to downsample by borough, by month. \n",
        "final_df['incident_month'] = final_df['incident_date_time'].dt.month\n",
        "boroughs = final_df['borough_desc'].unique()\n",
        "months = final_df['incident_month'].unique()\n",
        "\n",
        "#Event count distribution prior to sampling\n",
        "\n",
        "sns.countplot(x=\"borough_desc\", hue='incident_month', palette=\"Blues_d\", data=final_df)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiNq3uNutPIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9v0OoB1e47A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#Drop the cells with missing values that we are choosing not to impute\n",
        "drop_cols =[  'aes_presence_desc'\n",
        "            , 'action_taken2_desc'\n",
        "            , 'action_taken3_desc'\n",
        "#            , 'arrival_date_time' #Choosing not to drop after team discussion. \n",
        "            , 'co_detector_present_desc'\n",
        "            , 'detector_presence_desc'\n",
        "            , 'fire_origin_below_grade_flag'\n",
        "            , 'fire_spread_desc'\n",
        "            , 'standpipe_sys_present_flag'\n",
        "            , 'story_fire_origin_count'\n",
        "           ]\n",
        "\n",
        "final_df = final_df.drop(drop_cols, axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y7VwrMChSZX",
        "colab_type": "text"
      },
      "source": [
        "## Missing Values After Imputation and Drops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbu2zxwihWa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "msno.matrix(final_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA9H8PcGg1x2",
        "colab_type": "text"
      },
      "source": [
        "## Additional Exploratory Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf9yVC4E-X8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Begin Aditya's Code\n",
        "\n",
        "def groupByBurough_dwelling (df):\n",
        "   grouped = df.groupby('borough_desc')\n",
        "   for name,group in df.groupby('borough_desc'):\n",
        "               plt.figure(name)\n",
        "               sns.countplot(y=\"property_use_desc\",data=group,order=group.property_use_desc.value_counts().iloc[:5].index).set_title(name)\n",
        "   return\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def groupByBurough_callTime (df):\n",
        "   for name,group in df.groupby('borough_desc'):\n",
        "       print(name)\n",
        "       df2=group.set_index('arrival_date_time')\n",
        "       ts_df=df2.loc[df2.index.notnull()]\n",
        "       ts_df.index = pd.to_datetime(ts_df.index)\n",
        "       ts_df['hour_of_call'] = ts_df.index.hour\n",
        "       plt.figure(name)\n",
        "       sns.countplot(x=\"hour_of_call\",data=ts_df).set_title(name)\n",
        "   return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pF5bZ_baVHaz",
        "colab": {}
      },
      "source": [
        "groupByBurough_dwelling(final_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfOhjs7-6yO-",
        "colab_type": "text"
      },
      "source": [
        "## References: \n",
        "\n",
        "[1] @ELECTRONIC{firedata,\n",
        "  author = {Fire Department of New York City (FDNY)},\n",
        "  year = {2019},\n",
        "  title = {Incidents Responded to by Fire Companies},\n",
        "  language = {English},\n",
        "  howpublished = {\\url=https://data.cityofnewyork.us/Public-Safety/Incidents-Responded-to-by-Fire-Companies/tm6d-hbzd},\n",
        "  owner = {NYC Open Data},\n",
        "  timestamp = {2019.03.21}\n",
        "}\n",
        "\n",
        "[2] @ELECTRONIC{fireboxdata,\n",
        "  author = {www.poi-factory.com)},\n",
        "  year = {2008},\n",
        "  title = {FDNY Firebox Locator Data},\n",
        "  language = {English},\n",
        "  howpublished = {\\url=http://www.poi-factory.com/node/11074},\n",
        "  owner = {POI Factory},\n",
        "  timestamp = {2008.02.26}\n",
        "}\n",
        "\n",
        "[3] @ELECTRONIC{firehousedata,\n",
        "  author = {Fire Department of New York City (FDNY))},\n",
        "  year = {2018},\n",
        "  title = {FDNY Firehouse Listing},\n",
        "  language = {English},\n",
        "  howpublished = {\\url=https://data.cityofnewyork.us/Public-Safety/FDNY-Firehouse-Listing/hc8x-tcnd},\n",
        "  owner = {NYC Open Data},\n",
        "  timestamp = {2018.09.10}\n",
        "}\n",
        "\n",
        "[4] @ELECTRONIC{firecodelist,\n",
        "  author = {FDNYnewyork.com},\n",
        "  year = {2018},\n",
        "  title = {FDNY Fire Alarm Code Listing and Dispatch Policy},\n",
        "  language = {English},\n",
        "  howpublished = {\\url=http://www.fdnewyork.com/aa.asp},\n",
        "  owner = {FDNYnewyork.com},\n",
        "  timestamp = {2019.09.12}"
      ]
    }
  ]
}